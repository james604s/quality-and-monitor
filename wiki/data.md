


* 系統能不能支援版本回滾？
資料處理邏輯有時候難免會更新，如果錯的話能不能快速回滾的之前的版本重跑資料？

* 紀錄任務進行時間
能不能量測每次的時間，如果有天處理時間異常增加能不能發送告警？

* 防止錯誤資料進入正式環境
資料處理有件有趣的事情是，即便程式邏輯正確，也不代表商業邏輯正確。通常一個新上線或是新修改後的資料處理程式，不會馬上將結果導入正式環境中以免污染當前的資料。如何透過環境設定切割來做到這件事，後面會有專文討論。

* 資源監控
能否簡單監控甚至預測所需資源？

* 易於開發/部署
開發 Data Pipeline 系統的人，和之後使用 Data Pipeline 建立任務的人可能是不同的兩群人（例如系統架構師開發系統、之後交由資料工程師建立任務）。那這套系統使用的語言能不能符合之後使用者主要的開發語言？或能不能允許其他語言來使用（例如 Python 或 Shell Script）。

* 自動化的維運工具
Data Pipeline 系統和其他任何軟體系統一樣都需要維運，越自動的維運機制以及越完整的自我修復功能，都能大大降低未來的負擔。

```
由於 ETL 的正確性涵蓋了兩部分：1. 原始資料的正確 以及 2. 商務邏輯的正確，所以一般來說，比較嚴謹的開發流程會是這樣子：

測試資料 => 測試資料處理程式 => 測試 DB（先確認資料處理程式正確）

測試資料 => 正式資料處理程式 => 測試 DB

正式資料 => 正式資料處理程式 => 測試 DB （確認資料處理程式能應付正式資料）

正式資料 => 正式資料處理程式 => 正式 DB

正式資料 => 正式資料處理程式 => 暫存區 => 確認資料品質 => 正式DB
其他變形應用
```